apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: sail-prediction-demo-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.22, pipelines.kubeflow.org/pipeline_compilation_time: '2023-08-10T19:16:01.983605',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "testing sample", "inputs":
      [{"name": "data_path", "type": "String"}], "name": "sail prediction demo pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.22}
spec:
  entrypoint: sail-prediction-demo-pipeline
  templates:
  - name: prep-test-data
    container:
      args: []
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' 'numpy' 'scikit-learn' 'matplotlib' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location 'pandas' 'numpy'
        'scikit-learn' 'matplotlib' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def prep_test_data():
            import numpy as np
            import pandas as pd
            import pickle
            import matplotlib.pyplot as plt

            with open(f'data/rfr_optuna.pkl', 'rb') as f:
                rfr_optuna = pickle.load(f)
            xtest = np.load(f'data/xtest.npy', allow_pickle=True)
            ypred = rfr_optuna.predict(xtest)
            np.save(f'data/ypred.npy', ypred)

            ytest = np.load(f'data/ytest.npy', allow_pickle=True)
            plt.figure(figsize=(25, 10))
            plt.plot(ytest, c = 'blue')
            plt.plot(ypred, c = 'red')
            plt.show()

        import argparse
        _parser = argparse.ArgumentParser(prog='Prep test data', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = prep_test_data(**_parsed_args)
      image: python:3.7
      volumeMounts:
      - {mountPath: '{{inputs.parameters.data_path}}', name: t-vol}
    inputs:
      parameters:
      - {name: data_path}
      - {name: t-vol-name}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas'' ''numpy'' ''scikit-learn''
          ''matplotlib'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
          --quiet --no-warn-script-location ''pandas'' ''numpy'' ''scikit-learn''
          ''matplotlib'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def prep_test_data():\n    import numpy as np\n    import pandas as pd\n    import
          pickle\n    import matplotlib.pyplot as plt\n\n    with open(f''data/rfr_optuna.pkl'',
          ''rb'') as f:\n        rfr_optuna = pickle.load(f)\n    xtest = np.load(f''data/xtest.npy'',
          allow_pickle=True)\n    ypred = rfr_optuna.predict(xtest)\n    np.save(f''data/ypred.npy'',
          ypred)\n\n    ytest = np.load(f''data/ytest.npy'', allow_pickle=True)\n    plt.figure(figsize=(25,
          10))\n    plt.plot(ytest, c = ''blue'')\n    plt.plot(ypred, c = ''red'')\n    plt.show()\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Prep test data'', description='''')\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = prep_test_data(**_parsed_args)\n"],
          "image": "python:3.7"}}, "name": "Prep test data"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/max_cache_staleness: POD}
    volumes:
    - name: t-vol
      persistentVolumeClaim: {claimName: '{{inputs.parameters.t-vol-name}}'}
  - name: prepare-data
    container:
      args: []
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' 'numpy' 'wget' 'matplotlib' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas' 'numpy' 'wget' 'matplotlib'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def prepare_data():
            import pandas as pd
            import numpy
            import wget
            # data = pd.read_csv("data/SAIL.NS.csv")
            url = "https://query1.finance.yahoo.com/v7/finance/download/SAIL.NS?period1=820454400&period2=1689811200&interval=1d&events=history&includeAdjustedClose=true"
            test = wget.download(url)
            data = pd.read_csv(test)
            data['year'] = pd.to_datetime(data['Date']).dt.year
            data['month'] = pd.to_datetime(data['Date']).dt.month
            data['day'] = pd.to_datetime(data['Date']).dt.day
            data = data.drop(['Date'], axis = 1)

            for c in data.columns:
                data[c].fillna(data[c].mean(), inplace=True)
            data.to_csv(f'data/final_data.csv', index = False)

        import argparse
        _parser = argparse.ArgumentParser(prog='Prepare data', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = prepare_data(**_parsed_args)
      image: python:3.7
      volumeMounts:
      - {mountPath: '{{inputs.parameters.data_path}}', name: t-vol}
    inputs:
      parameters:
      - {name: data_path}
      - {name: t-vol-name}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas'' ''numpy'' ''wget''
          ''matplotlib'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
          --quiet --no-warn-script-location ''pandas'' ''numpy'' ''wget'' ''matplotlib''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def prepare_data():\n    import pandas as pd\n    import numpy\n    import
          wget\n    # data = pd.read_csv(\"data/SAIL.NS.csv\")\n    url = \"https://query1.finance.yahoo.com/v7/finance/download/SAIL.NS?period1=820454400&period2=1689811200&interval=1d&events=history&includeAdjustedClose=true\"\n    test
          = wget.download(url)\n    data = pd.read_csv(test)\n    data[''year''] =
          pd.to_datetime(data[''Date'']).dt.year\n    data[''month''] = pd.to_datetime(data[''Date'']).dt.month\n    data[''day'']
          = pd.to_datetime(data[''Date'']).dt.day\n    data = data.drop([''Date''],
          axis = 1)\n\n    for c in data.columns:\n        data[c].fillna(data[c].mean(),
          inplace=True)\n    data.to_csv(f''data/final_data.csv'', index = False)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Prepare data'', description='''')\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = prepare_data(**_parsed_args)\n"],
          "image": "python:3.7"}}, "name": "Prepare data"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/max_cache_staleness: POD}
    volumes:
    - name: t-vol
      persistentVolumeClaim: {claimName: '{{inputs.parameters.t-vol-name}}'}
  - name: sail-prediction-demo-pipeline
    inputs:
      parameters:
      - {name: data_path}
    dag:
      tasks:
      - name: prep-test-data
        template: prep-test-data
        dependencies: [t-vol, training-rf-regressor]
        arguments:
          parameters:
          - {name: data_path, value: '{{inputs.parameters.data_path}}'}
          - {name: t-vol-name, value: '{{tasks.t-vol.outputs.parameters.t-vol-name}}'}
      - name: prepare-data
        template: prepare-data
        dependencies: [t-vol]
        arguments:
          parameters:
          - {name: data_path, value: '{{inputs.parameters.data_path}}'}
          - {name: t-vol-name, value: '{{tasks.t-vol.outputs.parameters.t-vol-name}}'}
      - {name: t-vol, template: t-vol}
      - name: train-test-split
        template: train-test-split
        dependencies: [prepare-data, t-vol]
        arguments:
          parameters:
          - {name: data_path, value: '{{inputs.parameters.data_path}}'}
          - {name: t-vol-name, value: '{{tasks.t-vol.outputs.parameters.t-vol-name}}'}
      - name: training-rf-regressor
        template: training-rf-regressor
        dependencies: [t-vol, train-test-split]
        arguments:
          parameters:
          - {name: data_path, value: '{{inputs.parameters.data_path}}'}
          - {name: t-vol-name, value: '{{tasks.t-vol.outputs.parameters.t-vol-name}}'}
  - name: t-vol
    resource:
      action: create
      manifest: |
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: '{{workflow.name}}-t-vol'
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 1Gi
    outputs:
      parameters:
      - name: t-vol-manifest
        valueFrom: {jsonPath: '{}'}
      - name: t-vol-name
        valueFrom: {jsonPath: '{.metadata.name}'}
      - name: t-vol-size
        valueFrom: {jsonPath: '{.status.capacity.storage}'}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: train-test-split
    container:
      args: []
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' 'numpy' 'scikit-learn' 'matplotlib' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location 'pandas' 'numpy'
        'scikit-learn' 'matplotlib' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def train_test_split():
            import pandas as pd
            import numpy as np
            from sklearn.preprocessing import MinMaxScaler
            data = pd.read_csv("data/final_data.csv")
            mms = MinMaxScaler(feature_range=(0,1))
            scaled_data = mms.fit_transform(data['Close'].values.reshape(-1,1))

            prediction_days = 60
            x_train = []
            y_train = []

            for i in range(prediction_days, len(scaled_data)):
                x_train.append(scaled_data[i - prediction_days: i, 0])
                y_train.append(scaled_data[i, 0])

            xdata, ydata = np.array(x_train), np.array(y_train)

            train_size = 0.66
            val_size = 0.2
            test_size = 0.2

            xtrain, ytrain = xdata[0:int(xdata.shape[0]*train_size)], ydata[0:int(ydata.shape[0]*train_size)]
            xval, yval = xdata[xtrain.shape[0]+1:int(xtrain.shape[0] + (xdata.shape[0]-xtrain.shape[0])*0.5)], ydata[ytrain.shape[0]+1:int(ytrain.shape[0] + (ydata.shape[0]-ytrain.shape[0])*0.5)]
            xtest, ytest = xdata[int(xtrain.shape[0]+xval.shape[0]+1):], ydata[int(ytrain.shape[0]+yval.shape[0]+1):]

            np.save(f'data/xtrain.npy',xtrain)
            np.save(f'data/xval.npy', xval)
            np.save(f'data/xtest.npy', xtest)
            np.save(f'data/ytrain.npy',ytrain)
            np.save(f'data/yval.npy', yval)
            np.save(f'data/ytest.npy', ytest)

        import argparse
        _parser = argparse.ArgumentParser(prog='Train test split', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_test_split(**_parsed_args)
      image: python:3.7
      volumeMounts:
      - {mountPath: '{{inputs.parameters.data_path}}', name: t-vol}
    inputs:
      parameters:
      - {name: data_path}
      - {name: t-vol-name}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas'' ''numpy'' ''scikit-learn''
          ''matplotlib'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
          --quiet --no-warn-script-location ''pandas'' ''numpy'' ''scikit-learn''
          ''matplotlib'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def train_test_split():\n    import pandas as pd\n    import numpy as np\n    from
          sklearn.preprocessing import MinMaxScaler\n    data = pd.read_csv(\"data/final_data.csv\")\n    mms
          = MinMaxScaler(feature_range=(0,1))\n    scaled_data = mms.fit_transform(data[''Close''].values.reshape(-1,1))\n\n    prediction_days
          = 60\n    x_train = []\n    y_train = []\n\n    for i in range(prediction_days,
          len(scaled_data)):\n        x_train.append(scaled_data[i - prediction_days:
          i, 0])\n        y_train.append(scaled_data[i, 0])\n\n    xdata, ydata =
          np.array(x_train), np.array(y_train)\n\n    train_size = 0.66\n    val_size
          = 0.2\n    test_size = 0.2\n\n    xtrain, ytrain = xdata[0:int(xdata.shape[0]*train_size)],
          ydata[0:int(ydata.shape[0]*train_size)]\n    xval, yval = xdata[xtrain.shape[0]+1:int(xtrain.shape[0]
          + (xdata.shape[0]-xtrain.shape[0])*0.5)], ydata[ytrain.shape[0]+1:int(ytrain.shape[0]
          + (ydata.shape[0]-ytrain.shape[0])*0.5)]\n    xtest, ytest = xdata[int(xtrain.shape[0]+xval.shape[0]+1):],
          ydata[int(ytrain.shape[0]+yval.shape[0]+1):]\n\n    np.save(f''data/xtrain.npy'',xtrain)\n    np.save(f''data/xval.npy'',
          xval)\n    np.save(f''data/xtest.npy'', xtest)\n    np.save(f''data/ytrain.npy'',ytrain)\n    np.save(f''data/yval.npy'',
          yval)\n    np.save(f''data/ytest.npy'', ytest)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Train test split'', description='''')\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = train_test_split(**_parsed_args)\n"],
          "image": "python:3.7"}}, "name": "Train test split"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/max_cache_staleness: POD}
    volumes:
    - name: t-vol
      persistentVolumeClaim: {claimName: '{{inputs.parameters.t-vol-name}}'}
  - name: training-rf-regressor
    container:
      args: []
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' 'numpy' 'scikit-learn' 'matplotlib' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location 'pandas' 'numpy'
        'scikit-learn' 'matplotlib' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def training_rf_regressor():
            import numpy as np
            import pandas as pd
            import pickle
            from sklearn.ensemble import RandomForestRegressor

            xtrain = np.load(f'data/xtrain.npy', allow_pickle=True)
            ytrain = np.load(f'data/ytrain.npy', allow_pickle=True)
            rfr_final = RandomForestRegressor(n_estimators=10, max_depth=10)
            rfr_final.fit(xtrain, ytrain)
            rfr_optuna = RandomForestRegressor(n_estimators=11, max_depth=57)
            rfr_optuna.fit(xtrain, ytrain)

            with open(f'data/rfr_final.pkl', 'wb') as f:
                pickle.dump(rfr_final, f)

            with open(f'data/rfr_optuna.pkl', 'wb') as f:
                pickle.dump(rfr_optuna, f)

        import argparse
        _parser = argparse.ArgumentParser(prog='Training rf regressor', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = training_rf_regressor(**_parsed_args)
      image: python:3.7
      volumeMounts:
      - {mountPath: '{{inputs.parameters.data_path}}', name: t-vol}
    inputs:
      parameters:
      - {name: data_path}
      - {name: t-vol-name}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas'' ''numpy'' ''scikit-learn''
          ''matplotlib'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
          --quiet --no-warn-script-location ''pandas'' ''numpy'' ''scikit-learn''
          ''matplotlib'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def training_rf_regressor():\n    import numpy as np\n    import pandas
          as pd\n    import pickle\n    from sklearn.ensemble import RandomForestRegressor\n\n    xtrain
          = np.load(f''data/xtrain.npy'', allow_pickle=True)\n    ytrain = np.load(f''data/ytrain.npy'',
          allow_pickle=True)\n    rfr_final = RandomForestRegressor(n_estimators=10,
          max_depth=10)\n    rfr_final.fit(xtrain, ytrain)\n    rfr_optuna = RandomForestRegressor(n_estimators=11,
          max_depth=57)\n    rfr_optuna.fit(xtrain, ytrain)\n\n    with open(f''data/rfr_final.pkl'',
          ''wb'') as f:\n        pickle.dump(rfr_final, f)\n\n    with open(f''data/rfr_optuna.pkl'',
          ''wb'') as f:\n        pickle.dump(rfr_optuna, f)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Training rf regressor'', description='''')\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = training_rf_regressor(**_parsed_args)\n"],
          "image": "python:3.7"}}, "name": "Training rf regressor"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/max_cache_staleness: POD}
    volumes:
    - name: t-vol
      persistentVolumeClaim: {claimName: '{{inputs.parameters.t-vol-name}}'}
  arguments:
    parameters:
    - {name: data_path}
  serviceAccountName: pipeline-runner
